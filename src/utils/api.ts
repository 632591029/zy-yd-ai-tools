export const API_CONFIG = {
  // ä½¿ç”¨å®é™…çš„ Workers URL
  graphqlUrl: process.env.REACT_APP_GRAPHQL_URL || 'https://zy-yd-ai-api.a632591029.workers.dev/graphql',
  // æ¨¡æ‹Ÿæ¨¡å¼å¼€å…³
  mockMode: process.env.REACT_APP_MOCK_MODE === 'true'
};

// GraphQL æŸ¥è¯¢
export const CHAT_MUTATION = `
  mutation SendMessage($input: ChatInput!) {
    sendMessage(input: $input) {
      success
      reply
      error
      usage {
        promptTokens
        completionTokens
        totalTokens
      }
    }
  }
`;

export const MODELS_QUERY = `
  query GetModels {
    models {
      id
      name
      provider
      description
    }
  }
`;

// AI æ¨¡å‹ç±»å‹
export interface AIModel {
  id: string;
  name: string;
  provider: string;
  description?: string;
}

// èŠå¤©å“åº”ç±»å‹
export interface ChatResponse {
  success: boolean;
  reply?: string;
  error?: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

// æ ¹æ®æ¨¡å‹è·å–å¹³è¡¡ä¼˜åŒ–å‚æ•°
function getOptimizedParams(model: string, temperature?: number, maxTokens?: number) {
  // DeepSeek æ¨¡å‹å¹³è¡¡ä¼˜åŒ–å‚æ•° - é€Ÿåº¦ä¸è´¨é‡å¹¶é‡
  if (model.includes('deepseek')) {
    return {
      temperature: temperature !== undefined ? temperature : 0.4, // é€‚ä¸­çš„éšæœºæ€§
      maxTokens: maxTokens !== undefined ? maxTokens : 800 // é€‚ä¸­çš„è¾“å‡ºé•¿åº¦
    };
  }
  
  // OpenAI æ¨¡å‹é»˜è®¤å‚æ•°
  return {
    temperature: temperature !== undefined ? temperature : 0.7,
    maxTokens: maxTokens !== undefined ? maxTokens : 1000
  };
}

// æ¨¡æ‹ŸAIå›å¤
function generateMockResponse(message: string, model: string): string {
  const responses = {
    'gpt-3.5-turbo': [
      `ä½œä¸ºGPT-3.5ï¼Œæˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼š"${message}"ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„è¯é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†åˆ†æä¸€ä¸‹...`,
      `æ ¹æ®æ‚¨çš„æé—®ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªé—®é¢˜æ¶‰åŠå‡ ä¸ªå…³é”®è¦ç‚¹ã€‚é¦–å…ˆ...`,
      `è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼åŸºäºæˆ‘çš„ç†è§£ï¼Œæˆ‘å»ºè®®ä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦æ¥è€ƒè™‘...`
    ],
    'gpt-4': [
      `ä½œä¸ºGPT-4ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´æ·±å…¥çš„åˆ†æã€‚æ‚¨æåˆ°çš„"${message}"ç¡®å®å€¼å¾—æ·±å…¥æ¢è®¨...`,
      `ä»GPT-4çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å…·æœ‰å¤šå±‚æ¬¡çš„å¤æ‚æ€§ã€‚è®©æˆ‘ä¸ºæ‚¨é€ä¸€åˆ†æ...`,
      `æ‚¨çš„é—®é¢˜å¾ˆæœ‰æ·±åº¦ã€‚ä½œä¸ºæ›´å…ˆè¿›çš„AIæ¨¡å‹ï¼Œæˆ‘è®¤ä¸ºåº”è¯¥ä»ç³»ç»Ÿæ€§çš„è§’åº¦æ¥å›ç­”...`
    ],
    'deepseek-chat': [
      `æ‚¨å¥½ï¼å…³äº"${message}"è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†è§£ç­”ã€‚`,
      `æˆ‘ç†è§£æ‚¨çš„è¯¢é—®ã€‚è®©æˆ‘ä¸ºæ‚¨æä¾›ä¸€ä¸ªå…¨é¢è€Œç®€æ´çš„å›ç­”ã€‚`,
      `å¥½çš„é—®é¢˜ï¼æˆ‘ä¼šä¸ºæ‚¨æä¾›å‡†ç¡®ä¸”æœ‰ç”¨çš„ä¿¡æ¯ã€‚`,
      `æ˜ç™½äº†ï¼è¿™ä¸ªé—®é¢˜ç¡®å®å€¼å¾—æ·±å…¥è®¨è®ºï¼Œè®©æˆ‘ä¸ºæ‚¨åˆ†æä¸€ä¸‹ã€‚`,
      `æ”¶åˆ°æ‚¨çš„é—®é¢˜ã€‚æˆ‘ä¼šç»™æ‚¨ä¸€ä¸ªæ¸…æ™°æ˜äº†çš„å›ç­”ã€‚`
    ],
    'deepseek-coder': [
      `å…³äºè¿™ä¸ªç¼–ç¨‹é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æä¾›ä¸€ä¸ªè¯¦ç»†çš„æŠ€æœ¯è§£ç­”ã€‚`,
      `æˆ‘æ¥å¸®æ‚¨è§£å†³è¿™ä¸ªä»£ç ç›¸å…³çš„é—®é¢˜ã€‚`,
      `è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ€æœ¯é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†åˆ†æã€‚`,
      `ä»æŠ€æœ¯è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è¿™æ ·è§£å†³ã€‚`
    ]
  };

  const modelResponses = responses[model as keyof typeof responses] || responses['deepseek-chat'];
  const randomResponse = modelResponses[Math.floor(Math.random() * modelResponses.length)];
  
  if (model.includes('deepseek')) {
    return `${randomResponse}\n\nè¿™æ˜¯ä¸€ä¸ªå¹³è¡¡ä¼˜åŒ–çš„å›ç­”ç¤ºä¾‹ï¼Œæ—¢ä¿è¯äº†å“åº”é€Ÿåº¦ï¼Œåˆä¿æŒäº†å†…å®¹çš„å®Œæ•´æ€§å’Œå®ç”¨æ€§ã€‚`;
  }
  
  return `${randomResponse}\n\nğŸ“ æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿå›å¤ï¼Œç”¨äºæ¼”ç¤ºä¸åŒAIæ¨¡å‹çš„å“åº”é£æ ¼ã€‚å®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨çœŸå®çš„APIã€‚`;
}

// GraphQL è¯·æ±‚å‡½æ•°
export async function graphqlRequest(query: string, variables?: any): Promise<any> {
  // å¦‚æœæ˜¯æ¨¡æ‹Ÿæ¨¡å¼
  if (API_CONFIG.mockMode) {
    console.log('ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼');
    
    // DeepSeekæ¨¡æ‹Ÿé€‚ä¸­çš„å“åº”æ—¶é—´
    const isDeepSeek = variables?.input?.model?.includes('deepseek');
    const delay = isDeepSeek ? 800 + Math.random() * 1200 : 1000 + Math.random() * 2000;
    
    await new Promise(resolve => setTimeout(resolve, delay));
    
    if (query.includes('models')) {
      return {
        models: [
          { id: 'deepseek-chat', name: 'DeepSeek Chat', provider: 'deepseek', description: 'DeepSeekçš„å¯¹è¯æ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)' },
          { id: 'deepseek-coder', name: 'DeepSeek Coder', provider: 'deepseek', description: 'DeepSeekçš„ä»£ç ç”Ÿæˆæ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)' },
          { id: 'gpt-3.5-turbo', name: 'GPT-3.5 Turbo', provider: 'openai', description: 'OpenAIçš„å¿«é€Ÿå“åº”æ¨¡å‹' },
          { id: 'gpt-4', name: 'GPT-4', provider: 'openai', description: 'OpenAIçš„æœ€å¼ºæ¨¡å‹' }
        ]
      };
    }
    
    if (query.includes('sendMessage') && variables?.input) {
      const { message, model } = variables.input;
      return {
        sendMessage: {
          success: true,
          reply: generateMockResponse(message, model),
          error: null,
          usage: {
            promptTokens: message.length / 4,
            completionTokens: isDeepSeek ? 120 : 150,
            totalTokens: message.length / 4 + (isDeepSeek ? 120 : 150)
          }
        }
      };
    }
  }

  // æ­£å¸¸APIè°ƒç”¨
  try {
    const response = await fetch(API_CONFIG.graphqlUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        query,
        variables,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    
    if (data.errors) {
      throw new Error(data.errors[0]?.message || 'GraphQL error');
    }

    return data.data;
  } catch (error) {
    console.error('GraphQL request failed:', error);
    throw error;
  }
}

// å‘é€èŠå¤©æ¶ˆæ¯ - å¹³è¡¡ä¼˜åŒ–ç‰ˆæœ¬
export async function sendChatMessage(
  message: string, 
  model: string = 'deepseek-chat',
  temperature?: number,
  maxTokens?: number
): Promise<string> {
  try {
    // æ ¹æ®æ¨¡å‹è·å–å¹³è¡¡ä¼˜åŒ–å‚æ•°
    const optimizedParams = getOptimizedParams(model, temperature, maxTokens);
    
    console.log('ğŸš€ Sending message with balanced params:', {
      model,
      optimizedParams,
      messageLength: message.length
    });

    const data = await graphqlRequest(CHAT_MUTATION, {
      input: {
        message,
        model,
        temperature: optimizedParams.temperature,
        maxTokens: optimizedParams.maxTokens,
      },
    });

    const result: ChatResponse = data.sendMessage;
    
    if (!result.success) {
      throw new Error(result.error || 'æœªçŸ¥é”™è¯¯');
    }

    return result.reply || 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆå›å¤ã€‚';
  } catch (error) {
    console.error('Send message failed:', error);
    throw new Error(error instanceof Error ? error.message : 'ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚');
  }
}

// è·å–å¯ç”¨æ¨¡å‹
export async function getAvailableModels(): Promise<AIModel[]> {
  try {
    const data = await graphqlRequest(MODELS_QUERY);
    return data.models;
  } catch (error) {
    console.error('Get models failed:', error);
    // è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
    return [
      {
        id: 'deepseek-chat',
        name: 'DeepSeek Chat',
        provider: 'deepseek',
        description: 'DeepSeekçš„å¯¹è¯æ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)'
      },
      {
        id: 'deepseek-coder',
        name: 'DeepSeek Coder',
        provider: 'deepseek',
        description: 'DeepSeekçš„ä»£ç ç”Ÿæˆæ¨¡å‹ (é€Ÿåº¦ä¼˜åŒ–)'
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'openai',
        description: 'OpenAIçš„å¿«é€Ÿå“åº”æ¨¡å‹'
      }
    ];
  }
}