export const API_CONFIG = {
  // ä½¿ç”¨å®é™…çš„ Workers URL
  graphqlUrl: process.env.REACT_APP_GRAPHQL_URL || 'https://zy-yd-ai-api.a632591029.workers.dev/graphql',
  // æ¨¡æ‹Ÿæ¨¡å¼å¼€å…³
  mockMode: process.env.REACT_APP_MOCK_MODE === 'true'
};

// GraphQL æŸ¥è¯¢
export const CHAT_MUTATION = `
  mutation SendMessage($input: ChatInput!) {
    sendMessage(input: $input) {
      success
      reply
      error
      usage {
        promptTokens
        completionTokens
        totalTokens
      }
    }
  }
`;

export const MODELS_QUERY = `
  query GetModels {
    models {
      id
      name
      provider
      description
    }
  }
`;

// AI æ¨¡å‹ç±»å‹
export interface AIModel {
  id: string;
  name: string;
  provider: string;
  description?: string;
}

// èŠå¤©å“åº”ç±»å‹
export interface ChatResponse {
  success: boolean;
  reply?: string;
  error?: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

// æ¨¡æ‹ŸAIå›å¤
function generateMockResponse(message: string, model: string): string {
  const responses = {
    'gpt-3.5-turbo': [
      `ä½œä¸ºGPT-3.5ï¼Œæˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼š"${message}"ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„è¯é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†åˆ†æä¸€ä¸‹...`,
      `æ ¹æ®æ‚¨çš„æé—®ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªé—®é¢˜æ¶‰åŠå‡ ä¸ªå…³é”®è¦ç‚¹ã€‚é¦–å…ˆ...`,
      `è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼åŸºäºæˆ‘çš„ç†è§£ï¼Œæˆ‘å»ºè®®ä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦æ¥è€ƒè™‘...`
    ],
    'gpt-4': [
      `ä½œä¸ºGPT-4ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´æ·±å…¥çš„åˆ†æã€‚æ‚¨æåˆ°çš„"${message}"ç¡®å®å€¼å¾—æ·±å…¥æ¢è®¨...`,
      `ä»GPT-4çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å…·æœ‰å¤šå±‚æ¬¡çš„å¤æ‚æ€§ã€‚è®©æˆ‘ä¸ºæ‚¨é€ä¸€åˆ†æ...`,
      `æ‚¨çš„é—®é¢˜å¾ˆæœ‰æ·±åº¦ã€‚ä½œä¸ºæ›´å…ˆè¿›çš„AIæ¨¡å‹ï¼Œæˆ‘è®¤ä¸ºåº”è¯¥ä»ç³»ç»Ÿæ€§çš„è§’åº¦æ¥å›ç­”...`
    ],
    'deepseek-chat': [
      `ä½œä¸ºDeepSeekå¯¹è¯æ¨¡å‹ï¼Œæˆ‘æ³¨æ„åˆ°æ‚¨çš„é—®é¢˜ï¼š"${message}"ã€‚ä»æŠ€æœ¯è§’åº¦æ¥çœ‹...`,
      `DeepSeekåœ¨è¿™ç±»é—®é¢˜ä¸Šæœ‰ç‹¬ç‰¹çš„è§è§£ã€‚è®©æˆ‘ä¸ºæ‚¨æä¾›ä¸€ä¸ªè¯¦ç»†çš„å›ç­”...`,
      `æ ¹æ®DeepSeekçš„è®­ç»ƒæ•°æ®å’Œç®—æ³•ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªé—®é¢˜å¯ä»¥è¿™æ ·ç†è§£...`
    ],
    'deepseek-coder': [
      `ä½œä¸ºDeepSeekç¼–ç¨‹ä¸“å®¶ï¼Œæˆ‘çœ‹åˆ°æ‚¨çš„é—®é¢˜ä¸æŠ€æœ¯ç›¸å…³ã€‚è®©æˆ‘ä¸ºæ‚¨æä¾›ä¸€ä¸ªæŠ€æœ¯æ€§çš„å›ç­”...`,
      `ä»ç¼–ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œ"${message}"è¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³...`,
      `ä½œä¸ºä¸“æ³¨äºä»£ç çš„AIåŠ©æ‰‹ï¼Œæˆ‘å»ºè®®é‡‡ç”¨ä»¥ä¸‹æŠ€æœ¯æ–¹æ¡ˆ...`
    ]
  };

  const modelResponses = responses[model as keyof typeof responses] || responses['gpt-3.5-turbo'];
  const randomResponse = modelResponses[Math.floor(Math.random() * modelResponses.length)];
  
  return `${randomResponse}\n\nğŸ“ æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿå›å¤ï¼Œç”¨äºæ¼”ç¤ºä¸åŒAIæ¨¡å‹çš„å“åº”é£æ ¼ã€‚å®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨çœŸå®çš„APIã€‚`;
}

// GraphQL è¯·æ±‚å‡½æ•°
export async function graphqlRequest(query: string, variables?: any): Promise<any> {
  // å¦‚æœæ˜¯æ¨¡æ‹Ÿæ¨¡å¼
  if (API_CONFIG.mockMode) {
    console.log('ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼');
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));
    
    if (query.includes('models')) {
      return {
        models: [
          { id: 'gpt-3.5-turbo', name: 'GPT-3.5 Turbo', provider: 'openai', description: 'OpenAIçš„å¿«é€Ÿå“åº”æ¨¡å‹ (æ¨¡æ‹Ÿ)' },
          { id: 'gpt-4', name: 'GPT-4', provider: 'openai', description: 'OpenAIçš„æœ€å¼ºæ¨¡å‹ (æ¨¡æ‹Ÿ)' },
          { id: 'deepseek-chat', name: 'DeepSeek Chat', provider: 'deepseek', description: 'DeepSeekçš„å¯¹è¯æ¨¡å‹ (æ¨¡æ‹Ÿ)' },
          { id: 'deepseek-coder', name: 'DeepSeek Coder', provider: 'deepseek', description: 'DeepSeekçš„ä»£ç ç”Ÿæˆæ¨¡å‹ (æ¨¡æ‹Ÿ)' }
        ]
      };
    }
    
    if (query.includes('sendMessage') && variables?.input) {
      const { message, model } = variables.input;
      return {
        sendMessage: {
          success: true,
          reply: generateMockResponse(message, model),
          error: null,
          usage: {
            promptTokens: message.length / 4,
            completionTokens: 150,
            totalTokens: message.length / 4 + 150
          }
        }
      };
    }
  }

  // æ­£å¸¸APIè°ƒç”¨
  try {
    const response = await fetch(API_CONFIG.graphqlUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        query,
        variables,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    
    if (data.errors) {
      throw new Error(data.errors[0]?.message || 'GraphQL error');
    }

    return data.data;
  } catch (error) {
    console.error('GraphQL request failed:', error);
    throw error;
  }
}

// å‘é€èŠå¤©æ¶ˆæ¯
export async function sendChatMessage(
  message: string, 
  model: string = 'gpt-3.5-turbo',
  temperature: number = 0.7,
  maxTokens: number = 1000
): Promise<string> {
  try {
    const data = await graphqlRequest(CHAT_MUTATION, {
      input: {
        message,
        model,
        temperature,
        maxTokens,
      },
    });

    const result: ChatResponse = data.sendMessage;
    
    if (!result.success) {
      throw new Error(result.error || 'æœªçŸ¥é”™è¯¯');
    }

    return result.reply || 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆå›å¤ã€‚';
  } catch (error) {
    console.error('Send message failed:', error);
    throw new Error(error instanceof Error ? error.message : 'ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚');
  }
}

// è·å–å¯ç”¨æ¨¡å‹
export async function getAvailableModels(): Promise<AIModel[]> {
  try {
    const data = await graphqlRequest(MODELS_QUERY);
    return data.models;
  } catch (error) {
    console.error('Get models failed:', error);
    // è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
    return [
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'openai',
        description: 'OpenAIçš„å¿«é€Ÿå“åº”æ¨¡å‹'
      }
    ];
  }
}